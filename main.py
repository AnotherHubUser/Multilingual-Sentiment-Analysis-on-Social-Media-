import pandas as pd
import numpy as np
import torch
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    pipeline, AutoModel
)
import tweepy
import praw
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from textblob import TextBlob
import emoji
import re
from langdetect import detect, DetectorFactory
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from collections import Counter, defaultdict
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

DetectorFactory.seed = 0

class MultilingualEmotionAnalyzer:
    """–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —ç–º–æ—Ü–∏–π –∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
    
    def __init__(self):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤
        self.sentiment_models = {
            'english': pipeline(
                "sentiment-analysis",
                model="cardiffnlp/twitter-roberta-base-sentiment-latest"
            ),
            'russian': pipeline(
                "sentiment-analysis", 
                model="blanchefort/rubert-base-cased-sentiment"
            ),
            'chinese': pipeline(
                "sentiment-analysis",
                model="uer/roberta-base-finetuned-chinanews-chinese"
            ),
            'multilingual': pipeline(
                "sentiment-analysis",
                model="nlptown/bert-base-multilingual-uncased-sentiment"
            )
        }
        
        # –ú–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —ç–º–æ—Ü–∏–π
        self.emotion_models = {
            'english': pipeline(
                "text-classification",
                model="j-hartmann/emotion-english-distilroberta-base",
                return_all_scores=True
            ),
            'multilingual': pipeline(
                "text-classification", 
                model="j-hartmann/emotion-english-distilroberta-base",
                return_all_scores=True
            )
        }
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä—ã
        self.vader = SentimentIntensityAnalyzer()
        
        # –ö—É–ª—å—Ç—É—Ä–Ω—ã–µ –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —ç–º–æ—Ü–∏–π
        self.cultural_modifiers = {
            'english': {
                'sarcasm_markers': ['yeah right', 'sure thing', 'oh great', 'wonderful'],
                'intensity_multiplier': 1.0,
                'politeness_weight': 0.3
            },
            'russian': {
                'sarcasm_markers': ['–Ω—É –¥–∞', '–∫–æ–Ω–µ—á–Ω–æ –∂–µ', '–∫–∞–∫ –∂–µ', '–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ'],
                'intensity_multiplier': 1.2,  # –†—É—Å—Å–∫–∏–µ –±–æ–ª–µ–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —ç–∫—Å–ø—Ä–µ—Å—Å–∏–≤–Ω—ã
                'politeness_weight': 0.2
            },
            'chinese': {
                'sarcasm_markers': ['ÂΩìÁÑ∂', 'Â§™Â•Ω‰∫Ü', 'ÁúüÁöÑÂêó'],
                'intensity_multiplier': 0.8,  # –ë–æ–ª–µ–µ —Å–¥–µ—Ä–∂–∞–Ω–Ω–∞—è –∫—É–ª—å—Ç—É—Ä–∞
                'politeness_weight': 0.5
            }
        }
        
        # –≠–º–æ–¥–∑–∏ –∞–Ω–∞–ª–∏–∑
        self.emotion_emojis = {
            'joy': ['üòä', 'üòÄ', 'üòÅ', 'üòÑ', 'üòÉ', 'üôÇ', 'üòã', 'üòÜ', 'üòÇ', 'ü§£'],
            'sadness': ['üò¢', 'üò≠', 'üòû', 'üòî', 'üòü', 'üòï', '‚òπÔ∏è', 'üôÅ'],
            'anger': ['üò†', 'üò°', 'ü§¨', 'üò§', 'üí¢', 'üëø'],
            'fear': ['üò®', 'üò∞', 'üò±', 'üôÄ', 'üòß'],
            'surprise': ['üòÆ', 'üòØ', 'üò≤', 'ü§Ø', 'üò≥'],
            'love': ['üòç', 'ü•∞', 'üòò', 'üíï', '‚ù§Ô∏è', 'üíñ', 'üíù']
        }
        
    def preprocess_social_media_text(self, text: str, language: str) -> str:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç–µ–π"""
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–∞–∂–Ω—ã—Ö —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤
        text = self.preserve_emotional_markers(text)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å–æ—Ü—Å–µ—Ç–µ–π
        text = re.sub(r'@\w+', '[USER]', text)  # –£–ø–æ–º–∏–Ω–∞–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        text = re.sub(r'#(\w+)', r'\1', text)   # –•–µ—à—Ç–µ–≥–∏ -> —Å–ª–æ–≤–∞
        text = re.sub(r'http\S+|www\S+', '[URL]', text)  # –°—Å—ã–ª–∫–∏
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Å–∏–º–≤–æ–ª–æ–≤ (–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏)
        text = re.sub(r'([!?.]){3,}', r'\1\1\1', text)
        text = re.sub(r'([a-zA-Z–∞-—è–ê-–Ø])\1{2,}', r'\1\1', text)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def preserve_emotional_markers(self, text: str) -> str:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤"""
        
        # –ó–∞–º–µ–Ω–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –Ω–∞ –ø–æ–ª–Ω—ã–µ —Ñ–æ—Ä–º—ã
        replacements = {
            'lol': 'laugh out loud',
            'omg': 'oh my god', 
            'wtf': 'what the f',
            'imho': 'in my humble opinion',
            '—Ç–ª–¥—Ä': '—Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ –Ω–µ —á–∏—Ç–∞–ª',
            '–∫–µ–∫': '—Å–º–µ—à–Ω–æ',
            '–ª–æ–ª': '—Å–º–µ—à–Ω–æ'
        }
        
        text_lower = text.lower()
        for abbrev, full_form in replacements.items():
            text_lower = text_lower.replace(abbrev, full_form)
            
        return text_lower
    
    def extract_emoji_emotions(self, text: str) -> Dict[str, int]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–æ—Ü–∏–π –∏–∑ —ç–º–æ–¥–∑–∏"""
        emoji_emotions = defaultdict(int)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö —ç–º–æ–¥–∑–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞
        emojis_in_text = [c for c in text if c in emoji.EMOJI_DATA]
        
        # –ü–æ–¥—Å—á–µ—Ç —ç–º–æ—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–º–æ–¥–∑–∏
        for emoji_char in emojis_in_text:
            for emotion, emoji_list in self.emotion_emojis.items():
                if emoji_char in emoji_list:
                    emoji_emotions[emotion] += 1
        
        return dict(emoji_emotions)
    
    def detect_sarcasm(self, text: str, language: str) -> float:
        """–î–µ—Ç–µ–∫—Ü–∏—è —Å–∞—Ä–∫–∞–∑–º–∞ —Å —É—á–µ—Ç–æ–º –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π"""
        
        sarcasm_score = 0.0
        text_lower = text.lower()
        
        # –ö—É–ª—å—Ç—É—Ä–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã —Å–∞—Ä–∫–∞–∑–º–∞
        cultural_markers = self.cultural_modifiers.get(language, {}).get('sarcasm_markers', [])
        
        for marker in cultural_markers:
            if marker in text_lower:
                sarcasm_score += 0.3
        
        # –û–±—â–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Å–∞—Ä–∫–∞–∑–º–∞
        # –ö–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
        contrast_patterns = [
            r'but\s+\w+',  # but really
            r'–∞\s+–Ω–∞\s+—Å–∞–º–æ–º\s+–¥–µ–ª–µ',  # –∞ –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ  
            r'–Ω–æ\s+–Ω–∞\s+—Å–∞–º–æ–º\s+–¥–µ–ª–µ',  # –Ω–æ –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ
            r'‰ΩÜÊòØÂÆûÈôÖ‰∏ä'  # –Ω–æ –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ (–∫–∏—Ç–∞–π—Å–∫–∏–π)
        ]
        
        for pattern in contrast_patterns:
            if re.search(pattern, text_lower):
                sarcasm_score += 0.2
        
        # –ß—Ä–µ–∑–º–µ—Ä–Ω—ã–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ —ç–ø–∏—Ç–µ—Ç—ã –≤ –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
        positive_words = ['amazing', 'wonderful', 'perfect', '–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ', '–æ—Ç–ª–∏—á–Ω–æ', 'ÂÆåÁæé']
        negative_context = ['not', 'never', 'no', '–Ω–µ', '–Ω–µ—Ç', '–Ω–∏–∫–æ–≥–¥–∞', '‰∏ç', 'Ê≤°']
        
        has_positive = any(word in text_lower for word in positive_words)
        has_negative = any(word in text_lower for word in negative_context)
        
        if has_positive and has_negative:
            sarcasm_score += 0.4
        
        # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        if re.search(r'[!?]{2,}', text):
            sarcasm_score += 0.1
            
        # –ö–∞–≤—ã—á–∫–∏ –≤–æ–∫—Ä—É–≥ —Å–ª–æ–≤
        if re.search(r'"[^"]*"', text):
            sarcasm_score += 0.2
            
        return min(sarcasm_score, 1.0)
    
    def analyze_comprehensive_sentiment(self, texts: List[str]) -> pd.DataFrame:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ —ç–º–æ—Ü–∏–π"""
        
        results = []
        
        for i, text in enumerate(texts):
            if not text or not text.strip():
                continue
                
            try:
                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞
                detected_lang = detect(text)
                lang_mapping = {'en': 'english', 'ru': 'russian', 'zh': 'chinese'}
                language = lang_mapping.get(detected_lang, 'multilingual')
                
                # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
                processed_text = self.preprocess_social_media_text(text, language)
                
                # –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
                sentiment_model = self.sentiment_models.get(language, self.sentiment_models['multilingual'])
                sentiment_result = sentiment_model(processed_text[:512])[0]
                
                # –ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–π
                emotion_model = self.emotion_models.get(language, self.emotion_models['multilingual'])
                emotion_results = emotion_model(processed_text[:512])[0]
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —ç–º–æ—Ü–∏–π –≤ —Å–ª–æ–≤–∞—Ä—å
                emotion_scores = {item['label']: item['score'] for item in emotion_results}
                dominant_emotion = max(emotion_scores.items(), key=lambda x: x[1])
                
                # VADER –∞–Ω–∞–ª–∏–∑ (—Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ –¥–ª—è –Ω–µ—Ñ–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞)
                vader_scores = self.vader.polarity_scores(processed_text)
                
                # –ê–Ω–∞–ª–∏–∑ —ç–º–æ–¥–∑–∏
                emoji_emotions = self.extract_emoji_emotions(text)
                
                # –î–µ—Ç–µ–∫—Ü–∏—è —Å–∞—Ä–∫–∞–∑–º–∞
                sarcasm_score = self.detect_sarcasm(processed_text, language)
                
                # –ö—É–ª—å—Ç—É—Ä–Ω–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞
                cultural_mod = self.cultural_modifiers.get(language, {})
                intensity_mult = cultural_mod.get('intensity_multiplier', 1.0)
                
                # –ö–æ–º–ø–æ–∑–∏—Ç–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–∏
                emotional_intensity = (
                    abs(sentiment_result['score'] - 0.5) * 2 * intensity_mult +
                    dominant_emotion[1] * intensity_mult +
                    sum(emoji_emotions.values()) * 0.1
                ) / 2
                
                result = {
                    'text_id': i,
                    'original_text': text[:200] + '...' if len(text) > 200 else text,
                    'processed_text': processed_text[:100] + '...' if len(processed_text) > 100 else processed_text,
                    'detected_language': language,
                    
                    # –û—Å–Ω–æ–≤–Ω–∞—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
                    'sentiment_label': sentiment_result['label'],
                    'sentiment_score': sentiment_result['score'],
                    'sentiment_confidence': sentiment_result['score'],
                    
                    # VADER —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
                    'vader_compound': vader_scores['compound'],
                    'vader_positive': vader_scores['pos'],
                    'vader_negative': vader_scores['neg'],
                    'vader_neutral': vader_scores['neu'],
                    
                    # –≠–º–æ—Ü–∏–∏
                    'dominant_emotion': dominant_emotion[0],
                    'dominant_emotion_score': dominant_emotion[1],
                    **{f'emotion_{k}': v for k, v in emotion_scores.items()},
                    
                    # –≠–º–æ–¥–∑–∏ —ç–º–æ—Ü–∏–∏
                    **{f'emoji_{k}': v for k, v in emoji_emotions.items()},
                    'total_emojis': sum(emoji_emotions.values()),
                    
                    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
                    'sarcasm_score': sarcasm_score,
                    'emotional_intensity': emotional_intensity,
                    'text_length': len(text),
                    'processed_length': len(processed_text),
                    
                    # –ö—É–ª—å—Ç—É—Ä–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
                    'cultural_intensity_modifier': intensity_mult,
                    'is_likely_sarcastic': sarcasm_score > 0.5
                }
                
                results.append(result)
                
            except Exception as e:
                print(f"Error processing text {i}: {str(e)[:100]}")
                continue
        
        return pd.DataFrame(results)
    
    def compare_cross_cultural_emotions(self, results_df: pd.DataFrame, 
                                      topic_keywords: List[str]) -> Dict:
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç–º–æ—Ü–∏–π –º–µ–∂–¥—É –∫—É–ª—å—Ç—É—Ä–∞–º–∏ –ø–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π —Ç–µ–º–µ"""
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ç–µ–º–µ
        topic_pattern = '|'.join(topic_keywords)
        topic_mask = results_df['original_text'].str.contains(topic_pattern, case=False, na=False)
        topic_data = results_df[topic_mask]
        
        if len(topic_data) == 0:
            return {'error': 'No data found for the specified topic'}
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —è–∑—ã–∫–∞–º
        cultural_comparison = {}
        
        for language in topic_data['detected_language'].unique():
            lang_data = topic_data[topic_data['detected_language'] == language]
            
            # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
            sentiment_stats = {
                'avg_sentiment_score': lang_data['sentiment_score'].mean(),
                'avg_vader_compound': lang_data['vader_compound'].mean(),
                'avg_emotional_intensity': lang_data['emotional_intensity'].mean(),
                'avg_sarcasm_score': lang_data['sarcasm_score'].mean(),
                'sample_size': len(lang_data)
            }
            
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —ç–º–æ—Ü–∏–π
            emotion_columns = [col for col in lang_data.columns if col.startswith('emotion_')]
            emotion_averages = {}
            
            for col in emotion_columns:
                emotion_name = col.replace('emotion_', '')
                emotion_averages[emotion_name] = lang_data[col].mean()
            
            # –¢–æ–ø –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏–µ —ç–º–æ—Ü–∏–∏
            dominant_emotions = lang_data['dominant_emotion'].value_counts().head().to_dict()
            
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —ç–º–æ–¥–∑–∏
            emoji_columns = [col for col in lang_data.columns if col.startswith('emoji_')]
            emoji_usage = {}
            
            for col in emoji_columns:
                emoji_type = col.replace('emoji_', '')
                emoji_usage[emoji_type] = lang_data[col].sum()
            
            cultural_comparison[language] = {
                'sentiment_statistics': sentiment_stats,
                'emotion_averages': emotion_averages,
                'dominant_emotions': dominant_emotions,
                'emoji_usage': emoji_usage,
                'cultural_insights': self.generate_cultural_insights(lang_data, language)
            }
        
        return cultural_comparison
    
    def generate_cultural_insights(self, lang_data: pd.DataFrame, language: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –∏–Ω—Å–∞–π—Ç–æ–≤"""
        insights = []
        
        # –ê–Ω–∞–ª–∏–∑ –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–∏ —ç–º–æ—Ü–∏–π
        avg_intensity = lang_data['emotional_intensity'].mean()
        if avg_intensity > 0.7:
            insights.append(f"{language} speakers show high emotional intensity in discussions")
        elif avg_intensity < 0.3:
            insights.append(f"{language} speakers tend to be more emotionally restrained")
        
        # –ê–Ω–∞–ª–∏–∑ —Å–∞—Ä–∫–∞–∑–º–∞
        avg_sarcasm = lang_data['sarcasm_score'].mean()
        if avg_sarcasm > 0.3:
            insights.append(f"High sarcasm usage detected in {language} posts")
        
        # –ê–Ω–∞–ª–∏–∑ —ç–º–æ–¥–∑–∏
        total_emojis = lang_data['total_emojis'].sum()
        if total_emojis > len(lang_data) * 0.5:
            insights.append(f"{language} speakers frequently use emojis to express emotions")
        
        # –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤
        avg_length = lang_data['text_length'].mean()
        if avg_length > 200:
            insights.append(f"{language} users tend to write longer, more detailed posts")
        elif avg_length < 100:
            insights.append(f"{language} users prefer concise, brief expressions")
        
        return insights
    
    def create_emotion_dashboard(self, results_df: pd.DataFrame) -> go.Figure:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–∞—à–±–æ—Ä–¥–∞ —ç–º–æ—Ü–∏–π"""
        
        # –°–æ–∑–¥–∞–Ω–∏–µ subplot'–æ–≤
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=['Sentiment Distribution by Language', 
                          'Emotion Intensity Comparison',
                          'Sarcasm vs Emotional Intensity', 
                          'Emoji Usage by Language'],
            specs=[[{"type": "bar"}, {"type": "box"}],
                   [{"type": "scatter"}, {"type": "bar"}]]
        )
        
        # 1. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —è–∑—ã–∫–∞–º
        sentiment_counts = results_df.groupby(['detected_language', 'sentiment_label']).size().unstack(fill_value=0)
        
        for i, lang in enumerate(sentiment_counts.index):
            fig.add_trace(
                go.Bar(
                    name=lang,
                    x=sentiment_counts.columns,
                    y=sentiment_counts.loc[lang],
                    text=sentiment_counts.loc[lang],
                    textposition='auto',
                ),
                row=1, col=1
            )
        
        # 2. Box plot —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–∏
        for lang in results_df['detected_language'].unique():
            lang_data = results_df[results_df['detected_language'] == lang]
            fig.add_trace(
                go.Box(
                    y=lang_data['emotional_intensity'],
                    name=lang,
                    boxpoints='outliers'
                ),
                row=1, col=2
            )
        
        # 3. Scatter plot: —Å–∞—Ä–∫–∞–∑–º vs —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å
        colors = {'english': 'blue', 'russian': 'red', 'chinese': 'green'}
        
        for lang in results_df['detected_language'].unique():
            lang_data = results_df[results_df['detected_language'] == lang]
            fig.add_trace(
                go.Scatter(
                    x=lang_data['sarcasm_score'],
                    y=lang_data['emotional_intensity'],
                    mode='markers',
                    name=lang,
                    marker=dict(color=colors.get(lang, 'gray'), opacity=0.6),
                    text=[text[:50] + '...' for text in lang_data['original_text']],
                    hovertemplate='%{text}<br>Sarcasm: %{x}<br>Intensity: %{y}<extra></extra>'
                ),
                row=2, col=1
            )
        
        # 4. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —ç–º–æ–¥–∑–∏
        emoji_cols = [col for col in results_df.columns if col.startswith('emoji_')]
        emoji_totals = {}
        
        for lang in results_df['detected_language'].unique():
            lang_data = results_df[results_df['detected_language'] == lang]
            emoji_totals[lang] = lang_data[emoji_cols].sum().sum()
        
        fig.add_trace(
            go.Bar(
                x=list(emoji_totals.keys()),
                y=list(emoji_totals.values()),
                name='Total Emojis',
                text=list(emoji_totals.values()),
                textposition='auto',
                marker_color=['lightblue', 'lightcoral', 'lightgreen'][:len(emoji_totals)]
            ),
            row=2, col=2
        )
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ layout
        fig.update_layout(
            height=800,
            title_text="Cross-Cultural Emotion Analysis Dashboard",
            title_x=0.5,
            showlegend=True
        )
        
        fig.update_xaxes(title_text="Sentiment", row=1, col=1)
        fig.update_yaxes(title_text="Count", row=1, col=1)
        
        fig.update_yaxes(title_text="Emotional Intensity", row=1, col=2)
        
        fig.update_xaxes(title_text="Sarcasm Score", row=2, col=1)
        fig.update_yaxes(title_text="Emotional Intensity", row=2, col=1)
        
        fig.update_xaxes(title_text="Language", row=2, col=2)
        fig.update_yaxes(title_text="Total Emojis", row=2, col=2)
        
        return fig


class SocialMediaDataCollector:
    """–°–±–æ—Ä—â–∏–∫ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç–µ–π"""
    
    def __init__(self, twitter_bearer_token: str = None, reddit_credentials: Dict = None):
        self.twitter_bearer_token = twitter_bearer_token
        self.reddit_credentials = reddit_credentials
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤
        if twitter_bearer_token:
            self.twitter_client = tweepy.Client(bearer_token=twitter_bearer_token)
        
        if reddit_credentials:
            self.reddit_client = praw.Reddit(**reddit_credentials)
    
    def collect_twitter_data(self, query: str, lang: str = None, max_results: int = 100) -> List[Dict]:
        """–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ Twitter"""
        
        if not self.twitter_client:
            return []
        
        tweets = []
        
        try:
            # –ü–æ–∏—Å–∫ —Ç–≤–∏—Ç–æ–≤
            search_results = self.twitter_client.search_recent_tweets(
                query=query,
                lang=lang,
                max_results=max_results,
                tweet_fields=['created_at', 'author_id', 'public_metrics', 'lang']
            )
            
            if search_results.data:
                for tweet in search_results.data:
                    tweets.append({
                        'text': tweet.text,
                        'created_at': tweet.created_at,
                        'author_id': tweet.author_id,
                        'language': tweet.lang,
                        'retweet_count': tweet.public_metrics['retweet_count'],
                        'like_count': tweet.public_metrics['like_count'],
                        'source': 'twitter'
                    })
        
        except Exception as e:
            print(f"Error collecting Twitter data: {e}")
        
        return tweets
    
    def collect_reddit_data(self, subreddit_name: str, query: str = None, limit: int = 100) -> List[Dict]:
        """–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ Reddit"""
        
        if not self.reddit_client:
            return []
        
        posts = []
        
        try:
            subreddit = self.reddit_client.subreddit(subreddit_name)
            
            if query:
                # –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É
                search_results = subreddit.search(query, limit=limit)
            else:
                # –ì–æ—Ä—è—á–∏–µ –ø–æ—Å—Ç—ã
                search_results = subreddit.hot(limit=limit)
            
            for post in search_results:
                posts.append({
                    'text': post.title + ' ' + (post.selftext or ''),
                    'created_at': pd.to_datetime(post.created_utc, unit='s'),
                    'author_id': str(post.author) if post.author else 'deleted',
                    'score': post.score,
                    'num_comments': post.num_comments,
                    'subreddit': subreddit_name,
                    'source': 'reddit'
                })
        
        except Exception as e:
            print(f"Error collecting Reddit data: {e}")
        
        return posts

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã
if __name__ == "__main__":
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
    analyzer = MultilingualEmotionAnalyzer()
    
    # –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    sample_texts = [
        # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ —Ç–µ–∫—Å—Ç—ã
        "I absolutely love this new AI technology! It's going to change everything üòç #AI #technology",
        "Great job on the economy... really wonderful how everything is falling apart üôÑ",
        "Feeling anxious about climate change. What can we do to help our planet? üòüüåç",
        
        # –†—É—Å—Å–∫–∏–µ —Ç–µ–∫—Å—Ç—ã  
        "–ö–∞–∫–∞—è –∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–∞—è –ø–æ–≥–æ–¥–∞ —Å–µ–≥–æ–¥–Ω—è! –î–æ–∂–¥—å, —Ö–æ–ª–æ–¥, –ø—Ä–æ—Å—Ç–æ –ø—Ä–µ–ª–µ—Å—Ç—å üòí",
        "–û—á–µ–Ω—å —Ä–∞–¥ –Ω–æ–≤—ã–º —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º –≤ –æ–±–ª–∞—Å—Ç–∏ –ò–ò! –ë—É–¥—É—â–µ–µ —É–∂–µ –∑–¥–µ—Å—å üöÄ",
        "–ü–µ—Ä–µ–∂–∏–≤–∞—é –∑–∞ —ç–∫–æ–ª–æ–≥–∏—é –Ω–∞—à–µ–π –ø–ª–∞–Ω–µ—Ç—ã. –ù—É–∂–Ω–æ —á—Ç–æ-—Ç–æ –¥–µ–ª–∞—Ç—å! üò∞üå±",
        
        # –ö–∏—Ç–∞–π—Å–∫–∏–µ —Ç–µ–∫—Å—Ç—ã
        "Ëøô‰∏™Êñ∞ÁöÑ‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÁúüÁöÑÂæàÊ£íÔºÅÊàëÂæàÂÖ¥Â•ã üòä",
        "Â§©Ê∞îÁúüÂ•ΩÂïäÔºåÂèà‰∏ãÈõ®‰∫Ü üòë", 
        "ÂØπÊ∞îÂÄôÂèòÂåñÊÑüÂà∞ÊãÖÂøßÔºåÊàë‰ª¨Â∫îËØ•ÂÅö‰∫õ‰ªÄ‰πà üòü"
    ]
    
    # –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ —ç–º–æ—Ü–∏–π
    print("–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ —ç–º–æ—Ü–∏–π...")
    results = analyzer.analyze_comprehensive_sentiment(sample_texts)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    results.to_csv('multilingual_emotion_analysis.csv', index=False, encoding='utf-8')
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: multilingual_emotion_analysis.csv")
    
    # –ö—Ä–æ—Å—Å-–∫—É–ª—å—Ç—É—Ä–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
    topic_keywords = ['AI', 'technology', '–ò–ò', '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è', '‰∫∫Â∑•Êô∫ËÉΩ', 'ÊäÄÊúØ']
    cultural_comparison = analyzer.compare_cross_cultural_emotions(results, topic_keywords)
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    print("\n=== –ö–†–û–°–°-–ö–£–õ–¨–¢–£–†–ù–û–ï –°–†–ê–í–ù–ï–ù–ò–ï –≠–ú–û–¶–ò–ô ===")
    for language, data in cultural_comparison.items():
        print(f"\n{language.upper()}:")
        print(f"  –°—Ä–µ–¥–Ω—è—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {data['sentiment_statistics']['avg_sentiment_score']:.3f}")
        print(f"  –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å: {data['sentiment_statistics']['avg_emotional_intensity']:.3f}")
        print(f"  –£—Ä–æ–≤–µ–Ω—å —Å–∞—Ä–∫–∞–∑–º–∞: {data['sentiment_statistics']['avg_sarcasm_score']:.3f}")
        print(f"  –†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏: {data['sentiment_statistics']['sample_size']}")
        
        if data['cultural_insights']:
            print("  –ö—É–ª—å—Ç—É—Ä–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:")
            for insight in data['cultural_insights']:
                print(f"    - {insight}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—à–±–æ—Ä–¥–∞
    print("\n–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–∞—à–±–æ—Ä–¥–∞...")
    dashboard_fig = analyzer.create_emotion_dashboard(results)
    dashboard_fig.write_html('emotion_analysis_dashboard.html')
    print("–î–∞—à–±–æ—Ä–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: emotion_analysis_dashboard.html")
